AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >
  Helping customers break down data silos with AWS Integration services
  
# More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst
Globals:
  Function:
    Timeout: 300

Parameters:
  CloudWatchLogLevel:
      Type: String
      Default: ERROR
      AllowedValues:
        - ALL
        - ERROR
        - FATAL
        - OFF

  AppFlowVeevaConnectorName:
    Description: The name of the veeva connector
    Type: String
    Default: "veeva" 

  AppFlowSalesforcePushAppFlowName:
    Description: The name of the AppFlow Flow to update data in sfdc. Currently this flow needs to be manually created do to limitation in AppFlow CFN.
    Type: String
    Default: "s3-to-sfdc-update-patient-data-json"

  AppFlowSalesforceConnectorName:
    Description: The name of the salesforce connector
    Type: String
    Default: "sfdc" 

  AppFlowSalesforceObject:
    Description: The name of the salesforce object we will ingest with AppFlow
    Type: String
    Default: "Patient__c"     

  AppFlowSalesForceFlowName: 
    Description: Name of the flow to be used to pull data from Salesforce
    Type: String
    Default: "sfdc-to-s3"      
    
  HealthLakeEndpoint: 
    Description: The public endipoint for Amazon Healthlake
    Type: String
    Default: ""    

  GlueDatabaseName:
    Description: Name of the Glue database to create.
    Type: String
    Default: sfdc-data
  # GlueTableName:
  #   Type: String
  #   Default: cfn-manual-table-flights-1

Resources:

  DatalakeBucket:
    Type: "AWS::S3::Bucket"

  AppFlowSFDCtoS3:
    Type: AWS::AppFlow::Flow
    Properties: 
      Description: String
      FlowName: !Ref AppFlowSalesForceFlowName
      Description: 'Downloads data about patients from Salesforce into a raw S3 datalake'
      TriggerConfig:
        TriggerType: OnDemand
      SourceFlowConfig:
        ConnectorType: Salesforce
        ConnectorProfileName: !Ref AppFlowSalesforceConnectorName
        SourceConnectorProperties:
          Salesforce:
            Object: !Ref AppFlowSalesforceObject
            EnableDynamicFieldUpdate: true
            IncludeDeletedRecords: false
      DestinationFlowConfigList:
      - ConnectorType: S3
        DestinationConnectorProperties:
          S3:
            BucketName: !Ref DatalakeBucket
            S3OutputFormatConfig:
              FileType: JSON
              AggregationConfig:
                aggregationType: None
      Tasks:
      - TaskType: Filter
        SourceFields:
        - Id
        ConnectorOperator:
          Salesforce: PROJECTION


      - TaskType: Map_all
        SourceFields: []
        TaskProperties:
        - Key: EXCLUDE_SOURCE_FIELDS_LIST
          Value: '[]'
        ConnectorOperator:
          Salesforce: NO_OP

# Create an AWS Glue database
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Ref GlueDatabaseName
        Description: Database to hold tables for salesforce data
#Create IAM Role assumed by the crawler. For demonstration, this role is given all permissions.
  GlueRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          -
            Effect: "Allow"
            Principal:
              Service:
                - "glue.amazonaws.com"
            Action:
              - "sts:AssumeRole"
      Path: "/"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole      
      Policies:
        - PolicyName: !Sub "AWSGlueServiceRole-${AWS::StackName}"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                Resource: !Join [ "/*", [!GetAtt DatalakeBucket.Arn, ""]]

  GlueCrawlerSalesforce:
    Type: AWS::Glue::Crawler
    Properties:
      Role: !GetAtt GlueRole.Arn
      #Classifiers: none, use the default classifier
      Description: AWS Glue crawler to crawl salesforce data
      #Schedule: none, use default run-on-demand
      DatabaseName: !Ref GlueDatabase
      Targets:
        S3Targets:
          #  S3 bucket with the flights data
          - Path: !Sub "s3://${DatalakeBucket}/${AppFlowSalesForceFlowName}/"
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "LOG"
      Configuration: "{\"Version\":1.0,\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"},\"Tables\":{\"AddOrUpdateBehavior\":\"MergeNewColumns\"}}}"


  AppFlowVeevaLambdaFunction:
    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction
    Properties:
      CodeUri: src/veeva/
      Handler: app.lambdaHandler
      Runtime: nodejs12.x
      Tracing: Active
      Environment:
        Variables:
          GlueCatalogTable: salesforce_patient_s3   
          GlueCatalogDatabase: appflow-datalake-demo
          GlueTableKey: patientid__c      
      MemorySize: 2048
      Policies: # For PRODUCTION you may want to recify the permission boundries. At the moment the lambda function will have access to all S3 buckets and SQS queues. 
        - AWSLambdaBasicExecutionRole
        - AmazonS3FullAccess
        - AmazonAthenaFullAccess
      Timeout: 300

  AppFlowTriggerFunction:
    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction
    Properties:
      CodeUri: src/appflow/
      Handler: app.lambdaHandler
      Runtime: nodejs12.x
      Tracing: Active
      Environment:
        Variables:
          FLOW_NAME: !Ref AppFlowSalesforcePushAppFlowName   
  
      MemorySize: 2048
      Policies: # For PRODUCTION you may want to recify the permission boundries. At the moment the lambda function will have access to all S3 buckets and SQS queues. 
        - AWSLambdaBasicExecutionRole
        - AmazonAppFlowFullAccess
      Timeout: 300

  AppFlowHealthlakeLambdaFunction:
      Type: AWS::Serverless::Function
      Properties:
        Handler: com.amazonaws.samples.AppFlowDataHandler::handleRequest
        CodeUri: src/healthlake/
        Runtime: java11
        MemorySize: 512
        Timeout: 300
        Tracing: Active
        Environment:
          Variables:
            SERVICE_NAME: AppFlowDataHandler
            HL_ENDPOINT: !Ref HealthLakeEndpoint

        Policies:
          Statement:
            - Effect: Allow
              Action:
                - 's3:GetObject'
              Resource:
                - 'arn:aws:s3:::*'
            - Effect: Allow
              Action:
                - "healthlake:*"
              Resource:
                - '*'

  # Definition of our AWS Step Function. The actual ASL is extracted in statemachine/RealWorldExample.asl.json file.          
  StateMachine:
    Type: AWS::Serverless::StateMachine
    DependsOn: ProcessingStateMachineLogGroup
    Properties:
      Name: !Sub ${AWS::StackName}-ETL
      DefinitionUri: src/statemachine/VeevaPoC.asl.json
      DefinitionSubstitutions:
        VeevaLambdaFunctionArn: !GetAtt AppFlowVeevaLambdaFunction.Arn
        HLLambdaFunctionArn: !GetAtt AppFlowHealthlakeLambdaFunction.Arn
        AppFlowTriggerLambda: !GetAtt AppFlowTriggerFunction.Arn

      Type: "STANDARD"  
      Logging:
        Destinations:
          - CloudWatchLogsLogGroup:
              LogGroupArn: !GetAtt ProcessingStateMachineLogGroup.Arn
              
        IncludeExecutionData: false
        Level: !Ref CloudWatchLogLevel 
      Role: !GetAtt StatesExecutionRole.Arn            
      
  # Execution Role to allow Step Function to InvokeLambda, Log data into CloudWatch, publish events in EvnetBridge          
  StatesExecutionRole:
    DependsOn: ProcessingStateMachineLogGroup
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - !Sub states.${AWS::Region}.amazonaws.com
                - !Sub states.amazonaws.com
            Action: "sts:AssumeRole"
      Path: "/"   
      Policies:
        - PolicyName: StatesLoggingPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogDelivery
                  - logs:GetLogDelivery
                  - logs:UpdateLogDelivery
                  - logs:DeleteLogDelivery
                  - logs:ListLogDeliveries
                  - logs:PutResourcePolicy
                  - logs:DescribeResourcePolicies
                  - logs:DescribeLogGroups
                Resource: "*"
                
        - PolicyName: StatesExecutionPolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !Join [ ":*", [!GetAtt AppFlowVeevaLambdaFunction.Arn, ""]]
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !Join [ ":*", [!GetAtt AppFlowHealthlakeLambdaFunction.Arn, ""]]                
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !Join [ ":*", [!GetAtt AppFlowTriggerFunction.Arn, ""]]                  
                
  # Define CloudWatch Log Group for State execution logs
  ProcessingStateMachineLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "stepfunctions/${AWS::StackName}-ETL"                
      
  LoggingBucket:
    Type: "AWS::S3::Bucket"
    
  LoggingBucketPolicy:
    Type: 'AWS::S3::BucketPolicy'
    Properties:
      Bucket: !Ref LoggingBucket
      PolicyDocument:
        Version: "2012-10-17"    
        Statement:
        - Effect: Allow
          Principal:
            Service: "cloudtrail.amazonaws.com"
          Action: "s3:GetBucketAcl"
          Resource: !Sub "arn:aws:s3:::${LoggingBucket}"
        - Effect: Allow
          Principal:
            Service: "cloudtrail.amazonaws.com"
          Action: "s3:PutObject"
          Resource: !Sub "arn:aws:s3:::${LoggingBucket}/AWSLogs/${AWS::AccountId}/*"
          Condition:
            StringEquals:
              "s3:x-amz-acl": "bucket-owner-full-control"

  DatalakeBucketPolicy:
    Type: "AWS::S3::BucketPolicy"
    Properties: 
      Bucket: !Ref DatalakeBucket
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Effect: Allow
          Principal:
            Service: appflow.amazonaws.com
          Action:
          - s3:PutObject
          - s3:AbortMultipartUpload
          - s3:ListMultipartUploadParts
          - s3:ListBucketMultipartUploads
          - s3:GetBucketAcl
          - s3:PutObjectAcl
          Resource:
          - !GetAtt DatalakeBucket.Arn
          - !Join
            - ''
            - - 'arn:aws:s3:::'
              - !Ref DatalakeBucket
              - /*      
    
  CloudTrail:
    Type: "AWS::CloudTrail::Trail"
    DependsOn:
      - LoggingBucketPolicy
    Properties:
      IsLogging: true
      S3BucketName: !Ref LoggingBucket 
      EventSelectors:
        - DataResources:
            - Type: "AWS::S3::Object"
              Values:
                - "arn:aws:s3:::"  # log data events for all S3 buckets
                - !Sub "${DatalakeBucket.Arn}/${AWS::StackName}" # log data events for the S3 bucket defined above
          IncludeManagementEvents: true
          ReadWriteType: WriteOnly

  EventRule: 
    Type: AWS::Events::Rule
    Properties: 
      Description: "EventRule"
      State: "ENABLED"
      EventPattern: 
        source: 
          - "aws.s3"
        detail: 
          eventName: 
            - "PutObject"
          requestParameters:
            bucketName: 
              - !Ref DatalakeBucket

      Targets: 
        - 
          Id: !Sub ${AWS::StackName}-ETL
          Arn: !GetAtt StateMachine.Arn
          RoleArn: !GetAtt EventsExecutionRole.Arn

  EventsExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - events.amazonaws.com
            Action: "sts:AssumeRole"
      Path: "/"
      Policies:
        - PolicyName: Amazon_EventBridge_Invoke_Step_Functions
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: 
                  - states:StartExecution
                Resource: !GetAtt StateMachine.Arn
  KSMKey: 
    Type: AWS::KMS::Key
    Properties:
      Description: KMS key to be used by AppFlow.
      EnableKeyRotation: true
      PendingWindowInDays: 20
      KeyPolicy:
        Version: '2012-10-17'
        Id: !Sub "${AWS::StackName}-KMS"
        Statement:
        - Sid: Enable IAM User Permissions
          Effect: Allow
          Principal:
            AWS: !Sub "arn:aws:iam::${AWS::AccountId}:root"
          Action: kms:*
          Resource: "*"        
        - Sid: Allow access through Amazon AppFlow for all principals in the account that
            are authorized to use Amazon AppFlow
          Effect: Allow
          Principal:
            AWS: "*"
          Action:
          - kms:Encrypt
          - kms:Decrypt
          - kms:ReEncrypt*
          - kms:GenerateDataKey*
          - kms:CreateGrant
          - kms:DescribeKey
          Resource: "*"
          Condition:
            StringEquals:
              kms:CallerAccount: !Sub "${AWS::AccountId}"
              kms:ViaService: !Sub "appflow.${AWS::Region}.amazonaws.com"

        - Sid: Allow access through S3 for all principals in the account that are authorized
            to use S3
          Effect: Allow
          Principal:
            AWS: "*"
          Action: kms:Decrypt
          Resource: "*"
          Condition:
            StringEquals:
              kms:CallerAccount: !Sub "${AWS::AccountId}"
              kms:ViaService: !Sub "s3.${AWS::Region}.amazonaws.com"
        - Sid: Allow access through SecretManager for all principals in the account that are
            authorized to use SecretManager
          Effect: Allow
          Principal:
            AWS: "*"
          Action:
          - kms:Encrypt
          - kms:Decrypt
          - kms:ReEncrypt*
          - kms:GenerateDataKey*
          - kms:CreateGrant
          - kms:DescribeKey
          Resource: "*"
          Condition:
            StringEquals:
              kms:CallerAccount: !Sub "${AWS::AccountId}"
              kms:ViaService: !Sub "secretsmanager.${AWS::Region}.amazonaws.com"

  # AppFlowSalesforcePush: 
  #   DependsOn: DatalakeBucket
  #   Type: AWS::AppFlow::Flow
  #   Properties: 
  #     FlowName: !Ref AppFlowSalesforcePushAppFlowName
  #     TriggerConfig:
  #       TriggerType: OnDemand
  #     SourceFlowConfig:
  #       ConnectorType: S3
  #       SourceConnectorProperties:
  #         S3:
  #           BucketName: !Ref DatalakeBucket
  #           BucketPrefix: aggregated
  #           S3InputFormatConfig:
  #             S3InputFileType: JSON
  #     DestinationFlowConfigList:
  #     - ConnectorType: Salesforce
  #       ConnectorProfileName: !Ref AppFlowSalesforceConnectorName
  #       DestinationConnectorProperties:
  #         Salesforce:
  #           Object: Patient__c
  #           WriteOperationType: INSERT

  #     Tasks:
  #     - TaskType: Filter
  #       SourceFields:
  #       - first_vaccine_applied__c
  #       - is_fully_vacinated__c
  #       - second_vaccine_applied__c
  #       - vaccine_date1__c
  #       - vaccine_date2__c
  #       ConnectorOperator:
  #         S3: PROJECTION
  #     - TaskType: Map
  #       SourceFields:
  #       - first_vaccine_applied__c
  #       TaskProperties:
  #         - Key: DESTINATION_DATA_TYPE
  #           Value: boolean
  #       DestinationField: First_Vaccine_Applied__c
  #       ConnectorOperator:
  #         S3: NO_OP
  #     - TaskType: Map
  #       SourceFields:
  #       - is_fully_vacinated__c
  #       TaskProperties:
  #         - Key: DESTINATION_DATA_TYPE
  #           Value: boolean
  #       DestinationField: Is_Fully_Vacinated__c
  #       ConnectorOperator:
  #         S3: NO_OP
  #     - TaskType: Map
  #       SourceFields:
  #       - second_vaccine_applied__c
  #       TaskProperties:
  #         - Key: DESTINATION_DATA_TYPE
  #           Value: boolean
  #       DestinationField: Second_Vaccine_Applied__c
  #       ConnectorOperator:
  #         S3: NO_OP
  #     - TaskType: Map
  #       SourceFields:
  #       - vaccine_date1__c
  #       TaskProperties:
  #         - Key: DESTINATION_DATA_TYPE
  #           Value: string
  #       DestinationField: Vaccine_Date1__c
  #       ConnectorOperator:
  #         S3: NO_OP
  #     - TaskType: Map
  #       SourceFields:
  #       - vaccine_date2__c
  #       TaskProperties:
  #         - Key: DESTINATION_DATA_TYPE
  #           Value: string
  #       DestinationField: Vaccine_Date2__c
  #       ConnectorOperator:
  #         S3: NO_OP

  AppFlowVeeva:
    DependsOn: DatalakeBucket
    Type: AWS::AppFlow::Flow
    Properties: 
      FlowName: !Sub ${AWS::StackName}-veeva
      Description: Flow to fetch data from Veeva and save into our S3 datalake.
      DestinationFlowConfigList:
      - ConnectorType: S3
        DestinationConnectorProperties:
          S3:
            BucketName: !Ref DatalakeBucket
            BucketPrefix: !Sub ${AWS::StackName}-veeva,
            S3OutputFormatConfig:
              FileType: JSON
      KMSArn: !GetAtt KSMKey.Arn
      SourceFlowConfig: 
          ConnectorProfileName: !Ref AppFlowVeevaConnectorName
          ConnectorType: "Veeva" 
          SourceConnectorProperties:
              Veeva: 
                DocumentType: "Reference"
                Object: "documents/types/reference__c"
      Tasks:
      - ConnectorOperator:
          Veeva: PROJECTION
        SourceFields:
        - id
        - reference_source__c
        TaskType: Filter
      - ConnectorOperator:
          Veeva: NO_OP
        DestinationField: id
        SourceFields:
        - id
        TaskProperties:
        - Key: DESTINATION_DATA_TYPE
          Value: id
        - Key: SOURCE_DATA_TYPE
          Value: id
        TaskType: Map
      - ConnectorOperator:
          Veeva: NO_OP
        DestinationField: reference_source__c
        SourceFields:
        - reference_source__c
        TaskProperties:
          - Key: DESTINATION_DATA_TYPE 
            Value: String
          - Key: SOURCE_DATA_TYPE
            Value:  String
        TaskType: Map
      TriggerConfig:
        TriggerType: OnDemand